{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPE5nr9yBTKX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers fastapi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "ARm9FOkeBUP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Messages(BaseModel):\n",
        "    message_a: str\n",
        "    message_b: Optional[str] = None\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "cos_dict = {}\n",
        "time_dict = {}"
      ],
      "metadata": {
        "id": "fYS6FaUm6e5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get('/')\n",
        "def index():\n",
        "    return {'message': 'This is the homepage of the API. Send POST Request to /get_similarity to obtain a cosine distance for message_a and message_b.'}\n",
        "\n",
        "\n",
        "@app.post(\"/get_similarity\")\n",
        "def find_similarity(messages: Messages):\n",
        "  time_start_similarity = time.time()\n",
        "\n",
        "  messages_id = \"some id\"\n",
        "  sentences = [\n",
        "      messages.message_a,\n",
        "      messages.message_b\n",
        "  ]\n",
        "\n",
        "  tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "  for sentence in sentences:\n",
        "      \n",
        "      new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n",
        "                                        truncation=True, padding='max_length',\n",
        "                                        return_tensors='pt')\n",
        "      tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "      tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "  tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "  tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "\n",
        "  outputs = model(**tokens)\n",
        "  embeddings = outputs.last_hidden_state\n",
        "\n",
        "  attention_mask = tokens['attention_mask']\n",
        "  mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "  masked_embeddings = embeddings * mask\n",
        "  summed = torch.sum(masked_embeddings, 1)\n",
        "  summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
        "  mean_pooled = summed / summed_mask\n",
        "\n",
        "\n",
        "  mean_pooled = mean_pooled.detach().numpy()\n",
        "  cosine_array = cosine_similarity(\n",
        "      [mean_pooled[0]],\n",
        "      mean_pooled[1:]\n",
        "  )\n",
        "\n",
        "  cosine_similarity_value = cosine_array.sum()/(len(sentences)-1)\n",
        "\n",
        "  time_dict[messages_id] = time.time() - time_start_similarity\n",
        "  cos_dict[messages_id] = cosine_similarity_value\n",
        "\n",
        "  return cosine_similarity_value"
      ],
      "metadata": {
        "id": "dgoelQ5c6mE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FOR TESTING THE MODEL\n",
        "\n",
        "test = Messages(message_a = \"Once upon a time\",\n",
        "                message_b = \"Twice upon a time\")\n",
        "\n",
        "print(find_similarity(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNPag297AjnI",
        "outputId": "d56f1f7a-a758-40ec-848b-df0f6d6b5de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9999998807907104\n"
          ]
        }
      ]
    }
  ]
}